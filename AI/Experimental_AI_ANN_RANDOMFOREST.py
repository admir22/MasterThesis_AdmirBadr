# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-mjGgq-21EN-6-ehRMpHIBh5KUrmCW5w
"""

import scipy.io
import pandas as pd
import numpy as np

# Charger le fichier MAT
file_path = "harmonics_data_instantaneousEXPADDD.mat"  # Remplace par le bon chemin
data = scipy.io.loadmat(file_path)

# Extraire les données
results_dataset = data['AD']  # Remplace 'A' par le bon nom de variable si nécessaire

# Convertir en DataFrame pandas
df = pd.DataFrame(results_dataset, columns=['Te', 'Nf', 'speed_i', 'Ia', 'Iq_2nd_inst', 'Iq_4th_inst'])

# Ajouter la colonne de sévérité (Nf * Ia)
df['severity'] = df['Nf'] * df['Ia']

# Grouper par Te, speed_i, Nf et Ia, puis calculer la moyenne et 2x l'écart type
df_stats = df.groupby(['Te', 'Nf', 'speed_i', 'Ia']).agg(
    moyenne_2nd=('Iq_2nd_inst', 'mean'),
    ecarttype_2nd=('Iq_2nd_inst', lambda x: 2 * np.std(x)),
    moyenne_4th=('Iq_4th_inst', 'mean'),
    ecarttype_4th=('Iq_4th_inst', lambda x: 2 * np.std(x))
).reset_index()

# Convertir en un seul tableau NumPy (pour format .mat)
results_array = df_stats.to_numpy()

# Sauvegarde dans un fichier .mat sous la variable "results"
output_file = "new_harmonics_stats.mat"
scipy.io.savemat(output_file, {'results': results_array})

# Message de confirmation
print(f"Fichier sauvegardé : {output_file}, avec les résultats dans la variable 'results'.")

import scipy.io
import pandas as pd
import numpy as np

# Charger le fichier MAT
file_path = "harmonics_data_nofault_instantaneousEXPCCC.mat"  # Remplace par le bon chemin
data = scipy.io.loadmat(file_path)

# Extraire les données
results_dataset = data['D']  # Remplace 'A' par le bon nom de variable si nécessaire

# Convertir en DataFrame pandas
df = pd.DataFrame(results_dataset, columns=['Te', 'speed_i', 'Iq_2nd_inst', 'Iq_4th_inst'])

# Ajouter la colonne de sévérité (Nf * Ia)
df['severity'] = 0

# Grouper par Te, speed_i, Nf et Ia, puis calculer la moyenne et 2x l'écart type
df_stats = df.groupby(['Te', 'speed_i']).agg(
    moyenne_2nd=('Iq_2nd_inst', 'mean'),
    ecarttype_2nd=('Iq_2nd_inst', lambda x: 2 * np.std(x)),
    moyenne_4th=('Iq_4th_inst', 'mean'),
    ecarttype_4th=('Iq_4th_inst', lambda x: 2 * np.std(x))
).reset_index()

# Convertir en un seul tableau NumPy (pour format .mat)
results_array = df_stats.to_numpy()

# Sauvegarde dans un fichier .mat sous la variable "results"
output_file = "new_nofault_harmonics_stats.mat"
scipy.io.savemat(output_file, {'results': results_array})

# Message de confirmation
print(f"Fichier sauvegardé : {output_file}, avec les résultats dans la variable 'results'.")

import scipy.io
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Load MAT files (make sure the path is correct)
file_bruit = "new_harmonics_stats.mat"  # Replace with correct path
file_nofault = "new_nofault_harmonics_stats.mat"  # Replace with correct path

data_bruit = scipy.io.loadmat(file_bruit)
data_nofault = scipy.io.loadmat(file_nofault)

# Extract data from "results" variable
results_bruit = data_bruit['results']
results_nofault = data_nofault['results']

# Convert matrices to pandas DataFrames
df_bruit = pd.DataFrame(results_bruit)
df_nofault = pd.DataFrame(results_nofault)

# Select useful columns
columns_bruit = [0, 2, 4, 5, 6, 7]  # Te, speed_i, mean2, std2, mean4, std4
columns_nofault = [0, 1, 2, 3, 4, 5]  # Te, speed_i, mean2, std2, mean4, std4

# Filter the data to keep only columns of interest
df_bruit_filtered = df_bruit.iloc[:, columns_bruit].copy()
df_nofault_filtered = df_nofault.iloc[:, columns_nofault].copy()

# Rename columns to avoid confusion
column_names = ["Te", "speed_i", "mean2", "std2", "mean4", "std4"]
df_bruit_filtered.columns = column_names
df_nofault_filtered.columns = column_names

# Add fault severity for faulty machines
Nf = df_bruit.iloc[:, 1]  # Nf
ia = df_bruit.iloc[:, 3]  # ia
severity_fault = (Nf * ia)
df_bruit_filtered["severity"] = severity_fault

# Add severity column as "0" for healthy machines
df_nofault_filtered["severity"] = 0

# Identify unique combinations of torque and speed in healthy machines
unique_conditions = df_nofault_filtered[["Te", "speed_i"]].drop_duplicates()

# Create subplots for each unique condition
fig, axes = plt.subplots(2, 2, figsize=(12, 10))
axes = axes.flatten()

for i, (te, speed) in enumerate(unique_conditions.values):
    ax = axes[i]

    # Filter data for this specific condition
    df_healthy_case = df_nofault_filtered[(df_nofault_filtered["Te"] == te) & (df_nofault_filtered["speed_i"] == speed)]
    df_faulty_case = df_bruit_filtered[(df_bruit_filtered["Te"] == te) & (df_bruit_filtered["speed_i"] == speed)]

    # Print points used for each scatter plot
    print(f"Scatter Plot {i+1} - Torque: {te}, Speed: {speed}")
    print("Healthy Machine:")
    print(df_healthy_case)
    print("Faulty Machine:")
    print(df_faulty_case)
    print("\n" + "-"*50 + "\n")

    # Skip if no data points
    if df_faulty_case.empty or df_healthy_case.empty:
        continue

    # Error bars for healthy machines (foreground)
    for j in range(len(df_healthy_case)):
        ax.errorbar(
            df_healthy_case["mean2"].iloc[j],
            df_healthy_case["mean4"].iloc[j],
            xerr=df_healthy_case["std2"].iloc[j],
            yerr=df_healthy_case["std4"].iloc[j],
            fmt='none',
            ecolor='red',
            alpha=1,
            zorder=4
        )

    # Scatter plot for healthy machines (red crosses)
    ax.scatter(
        df_healthy_case["mean2"],
        df_healthy_case["mean4"],
        color='red',
        marker='x',
        edgecolors="k",
        alpha=1,
        s=60,
        label="Healthy machine",
        zorder=5
    )

    # Scatter plot for faulty machines
    scatter_faulty = ax.scatter(
        df_faulty_case["mean2"],
        df_faulty_case["mean4"],
        c=df_faulty_case["severity"],
        cmap="plasma",
        edgecolors="k",
        alpha=0.7,
        label="Faulty machine",
        zorder=2
    )

    # Error bars for faulty machines
    for j in range(len(df_faulty_case)):
        ax.errorbar(
            df_faulty_case["mean2"].iloc[j],
            df_faulty_case["mean4"].iloc[j],
            xerr=df_faulty_case["std2"].iloc[j],
            yerr=df_faulty_case["std4"].iloc[j],
            fmt='none',
            ecolor=plt.cm.plasma(df_faulty_case["severity"].iloc[j] / df_faulty_case["severity"].max()),
            alpha=0.7,
            zorder=1
        )

    # Titles and labels
    ax.set_xlabel("Mean of the magnitudes of 2nd harmonic (dB)")
    ax.set_ylabel("Mean of the magnitudes of 4th harmonic (dB)")
    ax.set_title(f"Torque: {te} Nm, Speed: {speed} RPM")
    ax.legend()

# Adjust layout and add a global colorbar on the right
cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  # Position of the colorbar on the right
fig.colorbar(scatter_faulty, cax=cbar_ax, label="Fault Severity (Nf × if / in)")

plt.tight_layout(rect=[0, 0, 0.9, 1])  # Leave space for the colorbar
plt.show()

import scipy.io
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Load MAT files (make sure the path is correct)
file_bruit = "new_harmonics_stats.mat"  # Replace with correct path
file_nofault = "new_nofault_harmonics_stats.mat"  # Replace with correct path

data_bruit = scipy.io.loadmat(file_bruit)
data_nofault = scipy.io.loadmat(file_nofault)

# Extract data from "results" variable
results_bruit = data_bruit['results']
results_nofault = data_nofault['results']

# Convert matrices to pandas DataFrames
df_bruit = pd.DataFrame(results_bruit)
df_nofault = pd.DataFrame(results_nofault)

# Select useful columns
columns_bruit = [0, 2, 4, 5, 6, 7]  # Te, speed_i, mean2, std2, mean4, std4
columns_nofault = [0, 1, 2, 3, 4, 5]  # Te, speed_i, mean2, std2, mean4, std4

# Filter the data to keep only columns of interest
df_bruit_filtered = df_bruit.iloc[:, columns_bruit].copy()
df_nofault_filtered = df_nofault.iloc[:, columns_nofault].copy()

# Rename columns to avoid confusion
column_names = ["Te", "speed_i", "mean2", "std2", "mean4", "std4"]
df_bruit_filtered.columns = column_names
df_nofault_filtered.columns = column_names

# Add fault severity for faulty machines
Nf = df_bruit.iloc[:, 1]  # Nf
ia = df_bruit.iloc[:, 3]  # ia
severity_fault = (Nf * ia)
df_bruit_filtered["severity"] = severity_fault

# Add severity column as "0" for healthy machines
df_nofault_filtered["severity"] = 0

# Filter data to include only 0.00 Nm and 500 RPM or 0.00 Nm and 1000 RPM
df_bruit_filtered_500 = df_bruit_filtered[(df_bruit_filtered["Te"] == 0.00) & (df_bruit_filtered["speed_i"] == 500)]
df_bruit_filtered_1000 = df_bruit_filtered[(df_bruit_filtered["Te"] == 0.00) & (df_bruit_filtered["speed_i"] == 1000)]

df_nofault_filtered_500 = df_nofault_filtered[(df_nofault_filtered["Te"] == 0.00) & (df_nofault_filtered["speed_i"] == 500)]
df_nofault_filtered_1000 = df_nofault_filtered[(df_nofault_filtered["Te"] == 0.00) & (df_nofault_filtered["speed_i"] == 1000)]

# Create subplots for 500 RPM and 1000 RPM
fig, axes = plt.subplots(1, 2, figsize=(12, 6))

# 500 RPM plot
ax1 = axes[0]
ax1.scatter(
    df_nofault_filtered_500["mean2"],
    df_nofault_filtered_500["mean4"],
    color='red',
    marker='x',
    edgecolors="k",
    alpha=1,
    s=60,
    label="Healthy machine",
    zorder=5
)

scatter_faulty_500 = ax1.scatter(
    df_bruit_filtered_500["mean2"],
    df_bruit_filtered_500["mean4"],
    c=df_bruit_filtered_500["severity"],
    cmap="plasma",
    edgecolors="k",
    alpha=0.7,
    label="Faulty machine",
    zorder=2
)

# Error bars for healthy machines at 500 RPM
for j in range(len(df_nofault_filtered_500)):
    ax1.errorbar(
        df_nofault_filtered_500["mean2"].iloc[j],
        df_nofault_filtered_500["mean4"].iloc[j],
        xerr=df_nofault_filtered_500["std2"].iloc[j],
        yerr=df_nofault_filtered_500["std4"].iloc[j],
        fmt='none',
        ecolor='red',
        alpha=1,
        zorder=4
    )

# Error bars for faulty machines at 500 RPM
for j in range(len(df_bruit_filtered_500)):
    ax1.errorbar(
        df_bruit_filtered_500["mean2"].iloc[j],
        df_bruit_filtered_500["mean4"].iloc[j],
        xerr=df_bruit_filtered_500["std2"].iloc[j],
        yerr=df_bruit_filtered_500["std4"].iloc[j],
        fmt='none',
        ecolor=plt.cm.plasma(df_bruit_filtered_500["severity"].iloc[j] / df_bruit_filtered_500["severity"].max()),
        alpha=0.7,
        zorder=1
    )

ax1.set_xlabel("Mean of the magnitudes of 2nd harmonic (dB)")
ax1.set_ylabel("Mean of the magnitudes of 4th harmonic (dB)")
ax1.set_title("Torque: 0.00 Nm, Speed: 500 RPM")
ax1.legend()

# 1000 RPM plot
ax2 = axes[1]
ax2.scatter(
    df_nofault_filtered_1000["mean2"],
    df_nofault_filtered_1000["mean4"],
    color='red',
    marker='x',
    edgecolors="k",
    alpha=1,
    s=60,
    label="Healthy machine",
    zorder=5
)

scatter_faulty_1000 = ax2.scatter(
    df_bruit_filtered_1000["mean2"],
    df_bruit_filtered_1000["mean4"],
    c=df_bruit_filtered_1000["severity"],
    cmap="plasma",
    edgecolors="k",
    alpha=0.7,
    label="Faulty machine",
    zorder=2
)

# Error bars for healthy machines at 1000 RPM
for j in range(len(df_nofault_filtered_1000)):
    ax2.errorbar(
        df_nofault_filtered_1000["mean2"].iloc[j],
        df_nofault_filtered_1000["mean4"].iloc[j],
        xerr=df_nofault_filtered_1000["std2"].iloc[j],
        yerr=df_nofault_filtered_1000["std4"].iloc[j],
        fmt='none',
        ecolor='red',
        alpha=1,
        zorder=4
    )

# Error bars for faulty machines at 1000 RPM
for j in range(len(df_bruit_filtered_1000)):
    ax2.errorbar(
        df_bruit_filtered_1000["mean2"].iloc[j],
        df_bruit_filtered_1000["mean4"].iloc[j],
        xerr=df_bruit_filtered_1000["std2"].iloc[j],
        yerr=df_bruit_filtered_1000["std4"].iloc[j],
        fmt='none',
        ecolor=plt.cm.plasma(df_bruit_filtered_1000["severity"].iloc[j] / df_bruit_filtered_1000["severity"].max()),
        alpha=0.7,
        zorder=1
    )

ax2.set_xlabel("Mean of the magnitudes of 2nd harmonic (dB)")
ax2.set_ylabel("Mean of the magnitudes of 4th harmonic (dB)")
ax2.set_title("Torque: 0.00 Nm, Speed: 1000 RPM")
ax2.legend()

# Add colorbar for fault severity
cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  # Position of the colorbar on the right
fig.colorbar(scatter_faulty_500, cax=cbar_ax, label="Fault Severity (Nf × if / in)")

plt.tight_layout(rect=[0, 0, 0.9, 1])  # Leave space for the colorbar
plt.show()

import scipy.io
import pandas as pd
import numpy as np
from scipy.spatial import distance
import matplotlib.pyplot as plt

# Charger les fichiers MAT (assure-toi de bien spécifier le bon chemin)
file_bruit = "new_harmonics_stats.mat"  # Remplace par le bon chemin
file_nofault = "new_nofault_harmonics_stats.mat"  # Remplace par le bon chemin

data_bruit = scipy.io.loadmat(file_bruit)
data_nofault = scipy.io.loadmat(file_nofault)

# Extraire les données de la variable "results"
results_bruit = data_bruit['results']
results_nofault = data_nofault['results']

# Convertir les matrices en DataFrames pandas
df_bruit = pd.DataFrame(results_bruit)
df_nofault = pd.DataFrame(results_nofault)

# Sélection des colonnes utiles
columns_bruit = [0, 2, 4, 5, 6, 7]  # Te, speed_i, moyenne2, ecarttype2, moyenne4, ecarttype4
columns_nofault = [0, 1, 2, 3, 4, 5]  # Te, speed_i, moyenne2, ecarttype2, moyenne4, ecarttype4

# Filtrage des données pour ne garder que les colonnes d'intérêt
df_bruit_filtered = df_bruit.iloc[:, columns_bruit].copy()
df_nofault_filtered = df_nofault.iloc[:, columns_nofault].copy()

# Renommer les colonnes pour éviter toute confusion
column_names = ["Te", "speed_i", "moyenne2", "ecarttype2", "moyenne4", "ecarttype4"]
df_bruit_filtered.columns = column_names
df_nofault_filtered.columns = column_names

# Ajouter la sévérité de la faute pour les machines avec défaut
Nf = df_bruit.iloc[:, 1]  # Nf
ia = df_bruit.iloc[:, 3]  # ia
severity_fault = (Nf * ia)
df_bruit_filtered["severity"] = severity_fault

# Ajouter une colonne de sévérité "0" pour les machines saines
df_nofault_filtered["severity"] = 0

# Filtrer les données pour ne garder que les cas avec 0.00 Nm de torque
df_bruit_filtered_0nm = df_bruit_filtered[df_bruit_filtered["Te"] == 0.00]
df_nofault_filtered_0nm = df_nofault_filtered[df_nofault_filtered["Te"] == 0.00]

# Identifier les différentes combinaisons de speed dans les machines saines
unique_conditions = df_nofault_filtered_0nm[["Te", "speed_i"]].drop_duplicates()

# Stocker les points suspects
df_suspects = pd.DataFrame()

# Identifier les points suspects et afficher les distances
for te, speed in unique_conditions.values:
    # Filtrer les données pour cette condition spécifique
    df_healthy_case = df_nofault_filtered_0nm[(df_nofault_filtered_0nm["speed_i"] == speed)]
    df_faulty_case = df_bruit_filtered_0nm[(df_bruit_filtered_0nm["speed_i"] == speed)]

    # Vérifier si on a des points correspondants
    if df_faulty_case.empty or df_healthy_case.empty:
        continue

    # Calcul des distances entre chaque point faulty et les points healthy
    distances = distance.cdist(df_faulty_case[["moyenne2", "ecarttype2", "moyenne4", "ecarttype4"]],
                               df_healthy_case[["moyenne2", "ecarttype2", "moyenne4", "ecarttype4"]],
                               metric='euclidean')

    # Création d'un DataFrame combiné avec distances et caractéristiques
    distance_df = pd.DataFrame(distances, index=df_faulty_case.index, columns=df_healthy_case.index)
    df_faulty_case = df_faulty_case.copy()
    df_faulty_case["min_distance_to_healthy"] = distance_df.min(axis=1)

    # Définition d'un seuil arbitraire
    threshold = 7  # Ajustable

    # Identifier les indices des points suspects
    min_distances = distances.min(axis=1)
    indices_faulty = df_faulty_case.index[min_distances < threshold]
    df_suspects = pd.concat([df_suspects, df_faulty_case.loc[indices_faulty]])

    # Vérification du contenu de df_suspects
    print(f"Nombre de points suspects après ajout pour Speed: {speed} = {len(df_suspects)}")

    # Affichage des distances et caractéristiques dans une seule table
    print(f"\033[1;34mDistances et caractéristiques pour Speed: {speed}\033[0m")
    print(df_faulty_case.to_string())
    print("\n" + "-"*50 + "\n")

# Vérification finale du contenu de df_suspects
print(f"\033[1;33mNombre total de points suspects identifiés : {len(df_suspects)}\033[0m")
print("\033[1;31mPoints suspects identifiés :\033[0m")
print(df_suspects.to_string())

import scipy.io
import pandas as pd
import numpy as np
from scipy.spatial import distance
import matplotlib.pyplot as plt

# Charger les fichiers MAT (assure-toi de bien spécifier le bon chemin)
file_bruit = "new_harmonics_stats.mat"  # Remplace par le bon chemin
file_nofault = "new_nofault_harmonics_stats.mat"  # Remplace par le bon chemin

data_bruit = scipy.io.loadmat(file_bruit)
data_nofault = scipy.io.loadmat(file_nofault)

# Extraire les données de la variable "results"
results_bruit = data_bruit['results']
results_nofault = data_nofault['results']

# Convertir les matrices en DataFrames pandas
df_bruit = pd.DataFrame(results_bruit)
df_nofault = pd.DataFrame(results_nofault)

# Sélection des colonnes utiles
columns_bruit = [0, 2, 4, 5, 6, 7]
columns_nofault = [0, 1, 2, 3, 4, 5]

# Filtrage des données pour ne garder que les colonnes d'intérêt
df_bruit_filtered = df_bruit.iloc[:, columns_bruit].copy()
df_nofault_filtered = df_nofault.iloc[:, columns_nofault].copy()

# Renommer les colonnes pour éviter toute confusion
column_names = ["Te", "speed_i", "moyenne2", "ecarttype2", "moyenne4", "ecarttype4"]
df_bruit_filtered.columns = column_names
df_nofault_filtered.columns = column_names

# Ajouter la sévérité de la faute pour les machines avec défaut
Nf = df_bruit.iloc[:, 1]
ia = df_bruit.iloc[:, 3]
severity_fault = (Nf * ia)
df_bruit_filtered["severity"] = severity_fault

# Ajouter une colonne de sévérité "0" pour les machines saines
df_nofault_filtered["severity"] = 0

# Filtrer les données pour ne garder que les cas avec 0.00 Nm de torque
df_bruit_filtered_0nm = df_bruit_filtered[df_bruit_filtered["Te"] == 0.00]
df_nofault_filtered_0nm = df_nofault_filtered[df_nofault_filtered["Te"] == 0.00]

# Identifier les différentes combinaisons de speed dans les machines saines
unique_conditions = df_nofault_filtered_0nm[["Te", "speed_i"]].drop_duplicates()

# Créer des sous-plots uniquement pour les conditions disponibles
fig, axes = plt.subplots(1, len(unique_conditions), figsize=(12, 6))
axes = axes.flatten()

# Si unique_conditions est vide, on évite d'essayer de créer un subplot
if len(unique_conditions) == 0:
    print("Aucune condition disponible pour 0.00 Nm de torque.")
else:
    for i, (te, speed) in enumerate(unique_conditions.values):
        ax = axes[i]

        df_healthy_case = df_nofault_filtered_0nm[(df_nofault_filtered_0nm["speed_i"] == speed)]
        df_faulty_case = df_bruit_filtered_0nm[(df_bruit_filtered_0nm["speed_i"] == speed)]
        df_suspect_case = df_suspects[(df_suspects["Te"] == 0.00) & (df_suspects["speed_i"] == speed)] if not df_suspects.empty else pd.DataFrame()

        # Si aucune donnée n'est présente pour cette condition, on saute cette itération
        if df_faulty_case.empty and df_healthy_case.empty and df_suspect_case.empty:
            continue

        # Machines saines : croix rouge + barres d'erreur
        ax.errorbar(
            df_healthy_case["moyenne2"],
            df_healthy_case["moyenne4"],
            xerr=df_healthy_case["ecarttype2"],
            yerr=df_healthy_case["ecarttype4"],
            fmt='x',
            color='red',
            ecolor='red',
            elinewidth=1,
            capsize=3,
            label="Healthy machine",
            zorder=4
        )

        # Machines avec défaut
        scatter_faulty = ax.scatter(
            df_faulty_case["moyenne2"],
            df_faulty_case["moyenne4"],
            c=df_faulty_case["severity"],
            cmap="plasma",
            edgecolors="k",
            alpha=0.7,
            label="Faulty machine",
            zorder=2
        )

        # Barres d'erreur des machines avec défaut
        for j in range(len(df_faulty_case)):
            ax.errorbar(
                df_faulty_case["moyenne2"].iloc[j],
                df_faulty_case["moyenne4"].iloc[j],
                xerr=df_faulty_case["ecarttype2"].iloc[j],
                yerr=df_faulty_case["ecarttype4"].iloc[j],
                fmt='none',
                ecolor=plt.cm.plasma(df_faulty_case["severity"].iloc[j] / df_faulty_case["severity"].max()),
                elinewidth=1,
                capsize=3
            )

        # Points suspects
        if not df_suspect_case.empty:
            ax.errorbar(
                df_suspect_case["moyenne2"],
                df_suspect_case["moyenne4"],
                xerr=df_suspect_case["ecarttype2"],
                yerr=df_suspect_case["ecarttype4"],
                fmt='o',
                color='cyan',
                ecolor='cyan',
                elinewidth=1,
                capsize=3,
                label="Suspect points",
                zorder=3
            )

        # Titres et labels (en anglais uniquement)
        ax.set_xlabel("Mean of the magnitudes of 2nd harmonic (dB)")
        ax.set_ylabel("Mean of the magnitudes of 4th harmonic (dB)")
        ax.set_title(f"Torque: {te} Nm, Speed: {speed} RPM")
        ax.legend()

    # Barre de couleur globale (en anglais)
    cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])
    fig.colorbar(scatter_faulty, cax=cbar_ax, label="Fault Severity (Nf × if / in)")

    plt.tight_layout(rect=[0, 0, 0.9, 1])
    plt.show()

import scipy.io
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Load MAT files (make sure the path is correct)
file_bruit = "new_harmonics_stats.mat"  # Replace with correct path
file_nofault = "new_nofault_harmonics_stats.mat"  # Replace with correct path

data_bruit = scipy.io.loadmat(file_bruit)
data_nofault = scipy.io.loadmat(file_nofault)

# Extract data from "results" variable
results_bruit = data_bruit['results']
results_nofault = data_nofault['results']

# Convert matrices to pandas DataFrames
df_bruit = pd.DataFrame(results_bruit)
df_nofault = pd.DataFrame(results_nofault)

# Select useful columns
columns_bruit = [0, 2, 4, 5, 6, 7]  # Te, speed_i, mean2, std2, mean4, std4
columns_nofault = [0, 1, 2, 3, 4, 5]  # Te, speed_i, mean2, std2, mean4, std4

# Filter the data to keep only columns of interest
df_bruit_filtered = df_bruit.iloc[:, columns_bruit].copy()
df_nofault_filtered = df_nofault.iloc[:, columns_nofault].copy()

# Rename columns to avoid confusion
column_names = ["Te", "speed_i", "mean2", "std2", "mean4", "std4"]
df_bruit_filtered.columns = column_names
df_nofault_filtered.columns = column_names

# Add fault severity for faulty machines
Nf = df_bruit.iloc[:, 1]  # Nf
ia = df_bruit.iloc[:, 3]  # ia
severity_fault = (Nf * ia)
df_bruit_filtered["severity"] = severity_fault

# Add severity column as "0" for healthy machines
df_nofault_filtered["severity"] = 0

# Identify unique combinations of torque and speed
unique_conditions = df_nofault_filtered[["Te", "speed_i"]].drop_duplicates()

# Create subplots for each unique condition (4 graphs)
fig, axes = plt.subplots(2, 2, figsize=(12, 10))  # 2x2 grid of subplots
axes = axes.flatten()

# Loop through each unique operating condition and plot the corresponding data
for i, (te, speed) in enumerate(unique_conditions.values):
    ax = axes[i]

    # Filter data for this specific condition
    df_healthy_case = df_nofault_filtered[(df_nofault_filtered["Te"] == te) & (df_nofault_filtered["speed_i"] == speed)]
    df_faulty_case = df_bruit_filtered[(df_bruit_filtered["Te"] == te) & (df_bruit_filtered["speed_i"] == speed)]

    # Skip if no data points
    if df_faulty_case.empty or df_healthy_case.empty:
        continue

    # Scatter plot for healthy machines (2nd harmonic - red crosses)
    ax.scatter(
        df_healthy_case["severity"],
        df_healthy_case["mean2"],
        color='red',
        marker='x',  # Cross for the 2nd harmonic
        edgecolors="k",
        alpha=1,
        s=60,
        label="Healthy machine (2nd harmonic)",
        zorder=5
    )

    # Scatter plot for healthy machines (4th harmonic - red circles)
    ax.scatter(
        df_healthy_case["severity"],
        df_healthy_case["mean4"],
        color='red',
        marker='o',  # Circle for the 4th harmonic
        edgecolors="k",
        alpha=1,
        s=60,
        label="Healthy machine (4th harmonic)",
        zorder=5
    )

    # Scatter plot for faulty machines (2nd harmonic - blue dots)
    ax.scatter(
        df_faulty_case["severity"],
        df_faulty_case["mean2"],
        color='blue',
        marker='o',  # Circle for the 2nd harmonic
        edgecolors="k",
        alpha=0.7,
        label="Faulty machine (2nd harmonic)",
        zorder=3
    )

    # Scatter plot for faulty machines (4th harmonic - green dots)
    ax.scatter(
        df_faulty_case["severity"],
        df_faulty_case["mean4"],
        color='green',
        marker='o',  # Circle for the 4th harmonic
        edgecolors="k",
        alpha=0.7,
        label="Faulty machine (4th harmonic)",
        zorder=3
    )

    # Titles and labels for each subplot
    ax.set_xlabel("Fault Severity (Nf × if / in)")
    ax.set_ylabel("Mean of Harmonics magnitudes (dB)")
    ax.set_title(f"Torque: {te} Nm, Speed: {speed} RPM")
    ax.legend()

plt.tight_layout()  # Adjust the layout to make room for the titles
plt.show()

import scipy.io
import pandas as pd
import numpy as np
from scipy.spatial import distance
import matplotlib.pyplot as plt

# Charger les fichiers MAT (assure-toi de bien spécifier le bon chemin)
file_bruit = "new_harmonics_stats.mat"  # Remplace par le bon chemin
file_nofault = "new_nofault_harmonics_stats.mat"  # Remplace par le bon chemin

data_bruit = scipy.io.loadmat(file_bruit)
data_nofault = scipy.io.loadmat(file_nofault)

# Extraire les données de la variable "results"
results_bruit = data_bruit['results']
results_nofault = data_nofault['results']

# Convertir les matrices en DataFrames pandas
df_bruit = pd.DataFrame(results_bruit)
df_nofault = pd.DataFrame(results_nofault)

# Sélection des colonnes utiles
columns_bruit = [0, 2, 4, 5, 6, 7]  # Te, speed_i, moyenne2, ecarttype2, moyenne4, ecarttype4
columns_nofault = [0, 1, 2, 3, 4, 5]  # Te, speed_i, moyenne2, ecarttype2, moyenne4, ecarttype4

# Filtrage des données pour ne garder que les colonnes d'intérêt
df_bruit_filtered = df_bruit.iloc[:, columns_bruit].copy()
df_nofault_filtered = df_nofault.iloc[:, columns_nofault].copy()

# Renommer les colonnes pour éviter toute confusion
column_names = ["Te", "speed_i", "moyenne2", "ecarttype2", "moyenne4", "ecarttype4"]
df_bruit_filtered.columns = column_names
df_nofault_filtered.columns = column_names

# Ajouter la sévérité de la faute pour les machines avec défaut
Nf = df_bruit.iloc[:, 1]  # Nf
ia = df_bruit.iloc[:, 3]  # ia
severity_fault = (Nf * ia)
df_bruit_filtered["severity"] = severity_fault

# Ajouter une colonne de sévérité "0" pour les machines saines
df_nofault_filtered["severity"] = 0

# Filtrer les données pour ne garder que les cas avec 0.00 Nm de torque
df_bruit_filtered_0nm = df_bruit_filtered[df_bruit_filtered["Te"] == 0.00]
df_nofault_filtered_0nm = df_nofault_filtered[df_nofault_filtered["Te"] == 0.00]

# Identifier les différentes combinaisons de speed dans les machines saines
unique_conditions = df_nofault_filtered_0nm[["Te", "speed_i"]].drop_duplicates()

# Supprimer les points suspects du dataset (si df_suspects existe)
if 'df_suspects' in globals():
    df_bruit_filtered_0nm = df_bruit_filtered_0nm[~df_bruit_filtered_0nm.index.isin(df_suspects.index)]

# Affichage du dataset mis à jour pour chaque condition unique
for te, speed in unique_conditions.values:
    df_case = df_bruit_filtered_0nm[(df_bruit_filtered_0nm["Te"] == 0.00) & (df_bruit_filtered_0nm["speed_i"] == speed)]
    print(f"\033[1;34mNouveau dataset après suppression des points suspects pour Torque: {te}, Speed: {speed}\033[0m")
    print(df_case.to_string())
    print("\n" + "-"*50 + "\n")

# Filtrer uniquement les cas avec Te = 0.00 Nm
df_bruit_filtered = df_bruit_filtered[df_bruit_filtered['Te'] == 0.00]
df_nofault_filtered = df_nofault_filtered[df_nofault_filtered['Te'] == 0.00]

# Identifier les différentes combinaisons uniques de speed_i dans les machines saines
unique_speeds = df_nofault_filtered['speed_i'].drop_duplicates()

# Créer les sous-plots pour chaque vitesse unique (te = 0.00 Nm)
fig, axes = plt.subplots(1, len(unique_speeds), figsize=(12, 6))
axes = axes.flatten()  # Si plus d'un subplot, ça garantit qu'ils sont dans un tableau 1D

for i, speed in enumerate(unique_speeds):
    ax = axes[i]

    # Filtrer les données pour cette condition spécifique de vitesse
    df_healthy_case = df_nofault_filtered[df_nofault_filtered["speed_i"] == speed]
    df_faulty_case = df_bruit_filtered[df_bruit_filtered["speed_i"] == speed]

    # Vérifier si on a des points correspondants
    if df_faulty_case.empty and df_healthy_case.empty:
        continue

    # Scatter plot des machines saines avec une croix rouge en avant-plan et barres d'erreur rouges
    ax.errorbar(
        df_healthy_case["moyenne2"],
        df_healthy_case["moyenne4"],
        xerr=df_healthy_case["ecarttype2"],
        yerr=df_healthy_case["ecarttype4"],
        fmt='x',
        color='red',
        ecolor='red',
        elinewidth=1,
        capsize=3,
        label="Healthy machine",
        zorder=3
    )

    # Scatter plot des machines avec défaut
    scatter_faulty = ax.scatter(
        df_faulty_case["moyenne2"],
        df_faulty_case["moyenne4"],
        c=df_faulty_case["severity"],
        cmap="plasma",
        edgecolors="k",
        alpha=0.7,
        label="Faulty machine",
        zorder=2
    )

    # Ajouter les barres d'erreur pour les machines avec défaut avec la même couleur que la sévérité
    for j in range(len(df_faulty_case)):
        ax.errorbar(
            df_faulty_case["moyenne2"].iloc[j],
            df_faulty_case["moyenne4"].iloc[j],
            xerr=df_faulty_case["ecarttype2"].iloc[j],
            yerr=df_faulty_case["ecarttype4"].iloc[j],
            fmt='none',
            ecolor=plt.cm.plasma(df_faulty_case["severity"].iloc[j] / df_faulty_case["severity"].max()),
            elinewidth=1,
            capsize=3
        )

    # Titres et labels
    ax.set_xlabel("Mean of the magnitudes of the 2nd harmonic (dB)")
    ax.set_ylabel("Mean of the magnitudes of the 4th harmonic  (dB)")
    ax.set_title(f"Torque: {te} Nm, Speed: {speed} RPM")
    ax.legend()

# Ajuster la mise en page et ajouter une barre de couleur globale
cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  # Position de la barre de couleur à droite
fig.colorbar(scatter_faulty, cax=cbar_ax, label="Fault severity (Nf * ia)")

plt.tight_layout(rect=[0, 0, 0.9, 1])  # Ajuste la mise en page pour laisser de la place à la barre de couleur
plt.show()

import scipy.io
import pandas as pd
import numpy as np
from scipy.spatial import distance
import matplotlib.pyplot as plt

# Charger les fichiers MAT (assure-toi de bien spécifier le bon chemin)
file_bruit = "new_harmonics_stats.mat"  # Remplace par le bon chemin
file_nofault = "new_nofault_harmonics_stats.mat"  # Remplace par le bon chemin
file_instantaneous = "harmonics_data_instantaneousEXPADDD.mat"  # Fichier contenant les valeurs instantanées

data_bruit = scipy.io.loadmat(file_bruit)
data_nofault = scipy.io.loadmat(file_nofault)
data_instantaneous = scipy.io.loadmat(file_instantaneous)

# Renommer les colonnes de df_bruit et df_nofault
df_bruit.columns = ["Te", "Nf", "speed_i", "moyenne2", "ecarttype2", "moyenne4", "ecarttype4", "severity"]  # Adapter selon la structure
df_nofault.columns = ["Te", "speed_i", "moyenne2", "ecarttype2", "moyenne4", "ecarttype4"]  # Adapter selon la structure

# Filtrer les données pour ne garder que les cas avec Te = 0.00 Nm
df_bruit_filtered = df_bruit[df_bruit['Te'] == 0.00].copy()
df_nofault_filtered = df_nofault[df_nofault['Te'] == 0.00].copy()
df_instantaneous_filtered = df_instantaneous[df_instantaneous['Te'] == 0.00].copy()



# Sélection des colonnes utiles
columns_bruit = [0, 2, 4, 5, 6, 7]  # Te, speed_i, moyenne2, ecarttype2, moyenne4, ecarttype4
columns_nofault = [0, 1, 2, 3, 4, 5]  # Te, speed_i, moyenne2, ecarttype2, moyenne4, ecarttype4

# Filtrage des données pour ne garder que les colonnes d'intérêt
df_bruit_filtered = df_bruit_filtered.iloc[:, columns_bruit].copy()
df_nofault_filtered = df_nofault_filtered.iloc[:, columns_nofault].copy()

# Renommer les colonnes pour éviter toute confusion
column_names = ["Te", "speed_i", "moyenne2", "ecarttype2", "moyenne4", "ecarttype4"]
df_bruit_filtered.columns = column_names
df_nofault_filtered.columns = column_names

# Ajouter la sévérité de la faute pour les machines avec défaut
Nf = df_bruit_filtered.iloc[:, 1]  # Nf
ia = df_bruit_filtered.iloc[:, 3]  # ia
severity_fault = (Nf * ia)
df_bruit_filtered["severity"] = severity_fault

# Ajouter la sévérité de la faute pour le dataset instantané
Nf = df_instantaneous_filtered.iloc[:, 1]  # Nf
ia = df_instantaneous_filtered.iloc[:, 3]  # ia
severity_fault_dataset = (Nf * ia)
df_instantaneous_filtered["severity"] = severity_fault_dataset

# Ajouter une colonne de sévérité "0" pour les machines saines
df_nofault_filtered["severity"] = 0

# Identifier les différentes combinaisons de torque et speed dans les machines saines
unique_conditions = df_nofault_filtered[["Te", "speed_i"]].drop_duplicates()

# Vérifier si df_suspects est vide
if df_suspects.empty:
    print("Aucun point suspect détecté. Aucun filtrage ne sera appliqué.")
else:
    # Supprimer les lignes du dataset instantané correspondant aux points suspects
    columns_to_match = ['Te', 'speed_i', 'severity']
    df_filtered_instantaneous = df_instantaneous_filtered.merge(df_suspects[columns_to_match], on=columns_to_match, how='left', indicator=True)
    df_filtered_instantaneous = df_filtered_instantaneous[df_filtered_instantaneous['_merge'] == 'left_only'].drop(columns=['_merge'])

    # Sauvegarde du dataset filtré
    filtered_file_path = "harmonics_data_filteredH.mat"
    scipy.io.savemat(filtered_file_path, {'dataset_filtered': df_filtered_instantaneous.to_numpy()})

    # Affichage du nombre de lignes supprimées
    print(f"Nombre de lignes supprimées : {len(df_instantaneous_filtered) - len(df_filtered_instantaneous)}")
    print("Nouveau dataset sans points suspects enregistré avec succès.")

import scipy.io
import pandas as pd
import numpy as np
from scipy.spatial import distance
import matplotlib.pyplot as plt

# Charger les fichiers MAT (assure-toi de bien spécifier le bon chemin)
file_bruit = "new_harmonics_stats.mat"  # Remplace par le bon chemin
file_nofault = "new_nofault_harmonics_stats.mat"  # Remplace par le bon chemin
file_instantaneous = "harmonics_data_filteredH.mat"  # Fichier contenant les valeurs instantanées

data_bruit = scipy.io.loadmat(file_bruit)
data_nofault = scipy.io.loadmat(file_nofault)
data_instantaneous = scipy.io.loadmat(file_instantaneous)

# Extraire les données de la variable "results"
results_bruit = data_bruit['results']
results_nofault = data_nofault['results']
results_dataset = data_instantaneous['dataset_filtered']

# Renommer les colonnes de df_bruit et df_nofault
df_bruit.columns = ["Te", "Nf", "speed_i", "moyenne2", "ecarttype2", "moyenne4", "ecarttype4", "severity"]  # Adapter selon la structure
df_nofault.columns = ["Te", "speed_i", "moyenne2", "ecarttype2", "moyenne4", "ecarttype4"]  # Adapter selon la structure
df_instantaneous = pd.DataFrame(results_dataset, columns=['Te', 'Nf', 'speed_i', 'Ia', 'Iq_2nd_inst', 'Iq_4th_inst', 'Severe'])

# Filtrer les données pour ne garder que les cas avec Te = 0.00 Nm et exclure Te = 9.65 Nm
df_bruit_filtered = df_bruit[(df_bruit['Te'] == 0.00) | (df_bruit['Te'] != 9.65)].copy()
df_nofault_filtered = df_nofault[(df_nofault['Te'] == 0.00) | (df_nofault['Te'] != 9.65)].copy()
df_instantaneous_filtered = df_instantaneous[(df_instantaneous['Te'] == 0.00) | (df_instantaneous['Te'] != 9.65)].copy()

# Sélection des colonnes utiles
columns_bruit = [0, 2, 4, 5, 6, 7]  # Te, speed_i, moyenne2, ecarttype2, moyenne4, ecarttype4
columns_nofault = [0, 1, 2, 3, 4, 5]  # Te, speed_i, moyenne2, ecarttype2, moyenne4, ecarttype4

# Filtrage des données pour ne garder que les colonnes d'intérêt
df_bruit_filtered = df_bruit_filtered.iloc[:, columns_bruit].copy()
df_nofault_filtered = df_nofault_filtered.iloc[:, columns_nofault].copy()

# Renommer les colonnes pour éviter toute confusion
column_names = ["Te", "speed_i", "moyenne2", "ecarttype2", "moyenne4", "ecarttype4"]
df_bruit_filtered.columns = column_names
df_nofault_filtered.columns = column_names

# Ajouter la sévérité de la faute pour les machines avec défaut
Nf = df_bruit.iloc[:, 1]  # Nf
ia = df_bruit.iloc[:, 3]  # ia
severity_fault = (Nf * ia)
df_bruit_filtered["severity"] = severity_fault

# Ajouter la sévérité de la faute pour le dataset instantané
Nf = df_instantaneous.iloc[:, 1]  # Nf
ia = df_instantaneous.iloc[:, 3]  # ia
severity_fault_dataset = (Nf * ia)
df_instantaneous_filtered["severity"] = severity_fault_dataset

# Ajouter une colonne de sévérité "0" pour les machines saines
df_nofault_filtered["severity"] = 0

# Filtrage des données en fonction des bornes définies
filtered_rows = []

for index, row in df_instantaneous_filtered.iterrows():
    te, speed, severity = row["Te"], row["speed_i"], row["severity"]
    matching_rows = df_bruit_filtered[(df_bruit_filtered["Te"] == te) &
                                      (df_bruit_filtered["speed_i"] == speed) &
                                      (df_bruit_filtered["severity"] == severity)]

    if not matching_rows.empty:
        mean_2nd = matching_rows["moyenne2"].values[0]
        std_2nd = matching_rows["ecarttype2"].values[0]
        mean_4th = matching_rows["moyenne4"].values[0]
        std_4th = matching_rows["ecarttype4"].values[0]

        if ((mean_2nd - std_2nd) <= row["Iq_2nd_inst"] <= (mean_2nd + std_2nd)) and \
           ((mean_4th - std_4th) <= row["Iq_4th_inst"] <= (mean_4th + std_4th)):
            filtered_rows.append(row)

# Création du DataFrame final
filtered_df = pd.DataFrame(filtered_rows)

# Sauvegarde du dataset filtré après cette nouvelle étape
filtered_file_path = "harmonics_data_refilteredH.mat"
scipy.io.savemat(filtered_file_path, {'dataset_refiltered': filtered_df.to_numpy()})

# Affichage du nombre de lignes restantes après filtrage
print(f"Nombre de lignes restantes après re-filtrage : {len(filtered_df)}")
print("Nouveau dataset re-filtré enregistré avec succès.")

import scipy.io
import pandas as pd
import numpy as np
from scipy.spatial import distance
import matplotlib.pyplot as plt

# Charger les fichiers MAT (assure-toi de bien spécifier le bon chemin)
file_bruit = "new_harmonics_stats.mat"  # Remplace par le bon chemin
file_nofault = "new_nofault_harmonics_stats.mat"  # Remplace par le bon chemin
file_nofault_instantaneous = "harmonics_data_nofault_instantaneousEXPCCC.mat"  # Fichier contenant les valeurs instantanées sans faute

data_bruit = scipy.io.loadmat(file_bruit)
data_nofault = scipy.io.loadmat(file_nofault)
data_nofault_instantaneous = scipy.io.loadmat(file_nofault_instantaneous)

# Extraire les données de la variable "results"
results_bruit = data_bruit['results']
results_nofault = data_nofault['results']
results_nofault_dataset = data_nofault_instantaneous['D']

# Convertir les matrices en DataFrames pandas
df_bruit = pd.DataFrame(results_bruit)
df_nofault = pd.DataFrame(results_nofault)
df_nofault_instantaneous = pd.DataFrame(results_nofault_dataset, columns=['Te', 'speed_i', 'Iq_2nd_inst', 'Iq_4th_inst'])

# Renommer les colonnes de df_bruit et df_nofault
df_bruit.columns = ["Te", "Nf", "speed_i", "moyenne2", "ecarttype2", "moyenne4", "ecarttype4", "severity"]
df_nofault.columns = ["Te", "speed_i", "moyenne2", "ecarttype2", "moyenne4", "ecarttype4"]


# Filtrer les données pour ne garder que les cas avec Te = 0.00 Nm et exclure Te = 9.65 Nm
df_bruit_filtered = df_bruit[(df_bruit['Te'] == 0.00) | (df_bruit['Te'] != 9.65)].copy()
df_nofault_filtered = df_nofault[(df_nofault['Te'] == 0.00) | (df_nofault['Te'] != 9.65)].copy()
df_nofault_instantaneous_filtered = df_nofault_instantaneous[(df_nofault_instantaneous['Te'] == 0.00) | (df_nofault_instantaneous['Te'] != 9.65)].copy()

# Sélection des colonnes utiles
columns_nofault = [0, 1, 2, 3, 4, 5]  # Te, speed_i, moyenne2, ecarttype2, moyenne4, ecarttype4

# Filtrage des données pour ne garder que les colonnes d'intérêt
df_nofault_filtered = df_nofault_filtered.iloc[:, columns_nofault].copy()

# Renommer les colonnes pour éviter toute confusion
column_names = ["Te", "speed_i", "moyenne2", "ecarttype2", "moyenne4", "ecarttype4"]
df_nofault_filtered.columns = column_names

# Filtrage des données en fonction des bornes définies
filtered_nofault_rows = []

for index, row in df_nofault_instantaneous_filtered.iterrows():
    te, speed = row["Te"], row["speed_i"]
    matching_rows = df_nofault_filtered[(df_nofault_filtered["Te"] == 0.00) &
                                        (df_nofault_filtered["speed_i"] == speed)]

    if not matching_rows.empty:
        mean_2nd = matching_rows["moyenne2"].values[0]
        std_2nd = matching_rows["ecarttype2"].values[0]
        mean_4th = matching_rows["moyenne4"].values[0]
        std_4th = matching_rows["ecarttype4"].values[0]

        if ((mean_2nd - std_2nd) <= row["Iq_2nd_inst"] <= (mean_2nd + std_2nd)) and \
           ((mean_4th - std_4th) <= row["Iq_4th_inst"] <= (mean_4th + std_4th)):
            filtered_nofault_rows.append(row)

# Création du DataFrame final pour les machines saines
filtered_nofault_df = pd.DataFrame(filtered_nofault_rows)

# Sauvegarde du dataset filtré après cette nouvelle étape
filtered_nofault_file_path = "harmonics_data_nofault_refiltered.mat"
scipy.io.savemat(filtered_nofault_file_path, {'dataset_nofault_refiltered': filtered_nofault_df.to_numpy()})

# Affichage du nombre de lignes restantes après filtrage
print(f"Nombre de lignes restantes après re-filtrage des machines saines : {len(filtered_nofault_df)}")
print("Nouveau dataset re-filtré pour machines saines enregistré avec succès.")

import scipy.io
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Charger les fichiers MAT
file_faulty_filtered = "harmonics_data_refilteredH.mat"  # Dataset filtré des machines avec faute
file_nofault_filtered = "harmonics_data_nofault_refiltered.mat"  # Dataset filtré des machines saines

# Charger les données
data_faulty_filtered = scipy.io.loadmat(file_faulty_filtered)
data_nofault_filtered = scipy.io.loadmat(file_nofault_filtered)

# Extraire les données
results_faulty_filtered = data_faulty_filtered['dataset_refiltered']
results_nofault_filtered = data_nofault_filtered['dataset_nofault_refiltered']

# Convertir les matrices en DataFrames pandas
df_faulty_filtered = pd.DataFrame(results_faulty_filtered, columns=['Te', 'Nf', 'speed_i', 'Ia', 'Iq_2nd_inst', 'Iq_4th_inst', 'Severe', 'severity'])
df_nofault_filtered = pd.DataFrame(results_nofault_filtered, columns=['Te', 'speed_i', 'Iq_2nd_inst', 'Iq_4th_inst'])

# Supprimer les valeurs NaN éventuelles
df_faulty_filtered.dropna(inplace=True)
df_nofault_filtered.dropna(inplace=True)

# Filtrer les données pour ne garder que celles avec Te = 0.00 Nm
df_faulty_filtered = df_faulty_filtered[df_faulty_filtered['Te'] == 0.00].copy()
df_nofault_filtered = df_nofault_filtered[df_nofault_filtered['Te'] == 0.00].copy()

# Identifier les différentes combinaisons de speed (500 RPM et 1000 RPM)
unique_speeds = df_nofault_filtered['speed_i'].drop_duplicates()

# Récupérer les valeurs min et max de sévérité pour la normalisation correcte de la barre de couleur
min_severity = df_faulty_filtered["severity"].min()
max_severity = df_faulty_filtered["severity"].max()

# Créer des sous-plots pour chaque vitesse unique
fig, axes = plt.subplots(1, len(unique_speeds), figsize=(12, 6))
axes = axes.flatten()

for i, speed in enumerate(unique_speeds):
    ax = axes[i]

    # Filtrer les données pour cette vitesse spécifique
    df_healthy_case = df_nofault_filtered[(df_nofault_filtered["speed_i"] == speed)]
    df_faulty_case = df_faulty_filtered[(df_faulty_filtered["speed_i"] == speed)]

    if df_faulty_case.empty and df_healthy_case.empty:
        continue

    # Scatter plot des machines saines en rouge avec croix
    ax.scatter(
        df_healthy_case["Iq_2nd_inst"],
        df_healthy_case["Iq_4th_inst"],
        color='red',
        marker='x',
        alpha=1,
        label="Healthy Machine",
        zorder=3
    )

    # Scatter plot des machines avec défaut avec échelle correcte pour la sévérité
    scatter_faulty = ax.scatter(
        df_faulty_case["Iq_2nd_inst"],
        df_faulty_case["Iq_4th_inst"],
        c=df_faulty_case["severity"],
        cmap="plasma",
        vmin=0,  # Assurer que la barre de couleur commence à 0
        vmax=50,  # Assurer qu'elle va bien jusqu'à 50
        edgecolors="k",
        alpha=0.7,
        label="Faulty machine",
        zorder=2
    )

    # Titres et labels
    ax.set_xlabel("Instantaneous magnitude values of the 2nd harmonic")
    ax.set_ylabel("Instantaneous magnitude values of the 4th harmonic")
    ax.set_title(f"Torque: 0.00 Nm, Speed: {speed} RPM")  # Ajout du couple (0.00 Nm) au titre
    ax.legend()

# Ajuster la mise en page et ajouter une barre de couleur globale sur le côté droit
cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  # Position de la barre sur le côté droit
fig.colorbar(scatter_faulty, cax=cbar_ax, label="Fault severity (Nf * if / in)")

plt.tight_layout(rect=[0, 0, 0.9, 1])  # Ajuste la mise en page pour laisser de la place à la barre de couleur
plt.show()

import scipy.io
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.tree import plot_tree

# 📂 Charger les fichiers MAT
file_faulty = "harmonics_data_refilteredH.mat"
file_healthy = "harmonics_data_nofault_refiltered.mat"

data_faulty = scipy.io.loadmat(file_faulty)['dataset_refiltered']
data_healthy = scipy.io.loadmat(file_healthy)['dataset_nofault_refiltered']

# 📊 Convertir les matrices en DataFrames pandas
df_faulty = pd.DataFrame(data_faulty, columns=['Te', 'Nf', 'speed_i', 'Ia', 'Iq_2nd_inst', 'Iq_4th_inst', 'Severe', 'severity'])
df_healthy = pd.DataFrame(data_healthy, columns=['Te', 'speed_i', 'Iq_2nd_inst', 'Iq_4th_inst'])

# 🏷️ Ajouter labels: 1 pour faulty, 0 pour healthy
df_faulty['label'] = 1
df_healthy['label'] = 0

# 🔹 Fusionner les datasets
df_all = pd.concat([df_faulty, df_healthy], ignore_index=True)

# 🔍 Identifier les différentes combinaisons de torque et speed
unique_conditions = df_all[['Te', 'speed_i']].drop_duplicates()

# 📈 Stockage des performances
model_performance = {}

# 🎯 Préparation des sous-plots pour les arbres de décision
fig, axes = plt.subplots(2, 2, figsize=(20, 15))  # 4 plots (2x2)
axes = axes.flatten()

# 🔄 Boucle sur chaque combinaison unique de torque et speed
for idx, (te, speed) in enumerate(unique_conditions.values):
    print(f"\n🔵 Training Random Forest for Torque: {te}, Speed: {speed}\n")

    # 📌 Sélection des données correspondantes
    df_case = df_all[(df_all['Te'] == te) & (df_all['speed_i'] == speed)]
    if df_case.empty:
        continue

    X = df_case[['Iq_2nd_inst', 'Iq_4th_inst']]
    y = df_case['label']

    # Split 80/10/10
    X_train, X_temp, y_train, y_temp = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y)
    X_val, X_test, y_val, y_test = train_test_split(
        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)

    # 🌳 Entraînement du Random Forest (sur jeu d'entraînement uniquement)
    clf = RandomForestClassifier(n_estimators=100, max_depth=4, random_state=42, bootstrap=False)
    clf.fit(X_train, y_train)

    # 📊 Évaluation sur validation
    y_val_pred = clf.predict(X_val)
    val_accuracy = accuracy_score(y_val, y_val_pred)
    print(f"🧪 Validation Accuracy: {val_accuracy:.4f}")

    # 📊 Évaluation sur test
    y_test_pred = clf.predict(X_test)
    test_accuracy = accuracy_score(y_test, y_test_pred)
    conf_matrix = confusion_matrix(y_test, y_test_pred)
    class_report = classification_report(y_test, y_test_pred)

    # 💾 Stockage des résultats
    model_performance[(te, speed)] = {
        'val_accuracy': val_accuracy,
        'test_accuracy': test_accuracy,
        'confusion_matrix': conf_matrix,
        'classification_report': class_report
    }

    # 📢 Affichage des résultats
    print(f"🎯 Test Accuracy: {test_accuracy:.4f}")
    print("📊 Matrice de confusion:")
    print(conf_matrix)
    print("📑 Rapport de classification:")
    print(class_report)

    # 🎨 Affichage d'un arbre de décision pour ce cas
    ax = axes[idx]
    plot_tree(clf.estimators_[0],
              feature_names=['Iq_2nd_inst', 'Iq_4th_inst'],
              class_names=['Healthy', 'Faulty'],
              filled=True,
              ax=ax)
    ax.set_title(f"Decision Tree (Torque={te}, Speed={speed})", fontsize=12)

plt.tight_layout()
plt.show()

print("\n✅ Expérimentation Random Forest terminée avec validation pour tous les cas.")

import scipy.io
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# 📂 Charger les fichiers MAT
file_faulty = "harmonics_data_refilteredH.mat"
file_healthy = "harmonics_data_nofault_refiltered.mat"

data_faulty = scipy.io.loadmat(file_faulty)['dataset_refiltered']
data_healthy = scipy.io.loadmat(file_healthy)['dataset_nofault_refiltered']

# 📊 Convertir les matrices en DataFrames pandas
df_faulty = pd.DataFrame(data_faulty, columns=['Te', 'Nf', 'speed_i', 'Ia', 'Iq_2nd_inst', 'Iq_4th_inst', 'Severe', 'severity'])
df_healthy = pd.DataFrame(data_healthy, columns=['Te', 'speed_i', 'Iq_2nd_inst', 'Iq_4th_inst'])

# 🏷️ Ajouter labels: 1 pour faulty, 0 pour healthy
df_faulty['label'] = 1
df_healthy['label'] = 0

# 🔹 Fusionner les datasets
df_all = pd.concat([df_faulty, df_healthy], ignore_index=True)

# 🔥 Filtrer les données pour ne garder que celles où Te == 0.00 Nm
df_all = df_all[df_all['Te'] == 0.00].copy()

# 🔍 Identifier les différentes combinaisons de speed (500 RPM et 1000 RPM)
unique_conditions = df_all[['Te', 'speed_i']].drop_duplicates()

# 📈 Stockage des performances
model_performance = {}

# 🎯 Préparation des sous-plots pour les matrices de confusion
fig, axes = plt.subplots(1, len(unique_conditions), figsize=(12, 10))  # Ajuster la taille en fonction du nombre de sous-graphiques
axes = axes.flatten()

# 🔄 Entraînement du modèle et stockage des matrices de confusion
for idx, (te, speed) in enumerate(unique_conditions.values):
    df_case = df_all[(df_all['Te'] == te) & (df_all['speed_i'] == speed)]
    if df_case.empty:
        continue

    X = df_case[['Iq_2nd_inst', 'Iq_4th_inst']]
    y = df_case['label']

    # Split 80/10/10
    X_train, X_temp, y_train, y_temp = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y)
    X_val, X_test, y_val, y_test = train_test_split(
        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)

    clf = RandomForestClassifier(n_estimators=50, max_depth=3, random_state=42, bootstrap=False)
    clf.fit(X_train, y_train)

    y_pred = clf.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    conf_matrix = confusion_matrix(y_test, y_pred)

    model_performance[(te, speed)] = {
        'accuracy': accuracy,
        'confusion_matrix': conf_matrix
    }

# 🎨 Affichage des matrices de confusion avec pourcentage uniquement et colormap sur les pourcentages
for idx, ((te, speed), results) in enumerate(model_performance.items()):
    cm = results['confusion_matrix']
    ax = axes[idx]

    # Normaliser la matrice de confusion en pourcentages (par ligne)
    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100

    # Appliquer la colormap sur les pourcentages
    im = ax.imshow(cm_percentage, interpolation='nearest', cmap=plt.cm.Blues)

    # Ajouter les pourcentages par ligne
    for i in range(cm_percentage.shape[0]):
        for j in range(cm_percentage.shape[1]):
            ax.text(j, i, f"{format(cm_percentage[i, j], '.2f')}%",
                    ha="center", va="center",
                    color="white" if cm_percentage[i, j] > 50 else "black",  # Changer la couleur du texte selon la luminosité
                    fontsize=12, fontweight='bold')

    # Configurer axes et titres
    ax.set(
        xticks=np.arange(2),
        yticks=np.arange(2),
        xticklabels=["0", "1"],
        yticklabels=["0", "1"]
    )
    ax.set_title(f"Confusion Matrix - Torque={te} Nm, Speed={speed} RPM", fontsize=12, fontweight="bold")
    ax.set_xlabel("Predicted class", fontsize=11, fontweight='bold')
    ax.set_ylabel("True class", fontsize=11, fontweight='bold')

plt.tight_layout()
plt.show()

import scipy.io
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# 📂 Charger les fichiers MAT
file_faulty = "harmonics_data_refilteredH.mat"
file_healthy = "harmonics_data_nofault_refiltered.mat"

data_faulty = scipy.io.loadmat(file_faulty)['dataset_refiltered']
data_healthy = scipy.io.loadmat(file_healthy)['dataset_nofault_refiltered']

# 📊 Convertir en DataFrame
df_faulty = pd.DataFrame(data_faulty, columns=['Te', 'Nf', 'speed_i', 'Ia',
                                               'Iq_2nd_inst', 'Iq_4th_inst', 'Severe', 'severity'])
df_healthy = pd.DataFrame(data_healthy, columns=['Te', 'speed_i', 'Iq_2nd_inst', 'Iq_4th_inst'])

df_faulty['label'] = df_faulty['Nf'] * df_faulty['Ia']
df_healthy['label'] = 0
df_healthy['severity'] = 0

df_all = pd.concat([df_faulty, df_healthy], ignore_index=True)

# Encoder les labels
encoder = LabelEncoder()
df_all['label'] = encoder.fit_transform(df_all['label'])

# 🔥 Filtrer les données pour ne garder que celles où Te == 0.00 Nm
df_all = df_all[df_all['Te'] == 0.00].copy()

# Identifier les cas uniques de speed (500 RPM et 1000 RPM)
unique_conditions = df_all[['Te', 'speed_i']].drop_duplicates()

# Préparation des plots
fig_cm, axes_cm = plt.subplots(1, 2, figsize=(12, 10))  # 2 plots (1x2 pour 500 RPM et 1000 RPM)
axes_cm = axes_cm.flatten()

fig_tree, axes_tree = plt.subplots(1, 2, figsize=(20, 15))  # 2 plots pour les arbres de décision
axes_tree = axes_tree.flatten()

for idx, (te, speed) in enumerate(unique_conditions.values):
    df_case = df_all[(df_all['Te'] == te) & (df_all['speed_i'] == speed)]
    if df_case.empty:
        continue

    # Équilibrer les données pour chaque classe
    min_per_label = df_case['label'].value_counts().min()
    df_balanced = df_case.groupby('label', group_keys=False).apply(
        lambda x: x.sample(min(len(x), min_per_label), random_state=42)).reset_index(drop=True)

    X = df_balanced[['Iq_2nd_inst', 'Iq_4th_inst']]
    y = df_balanced['label']

    # Split 80/10/10
    X_train, X_temp, y_train, y_temp = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y)
    X_val, X_test, y_val, y_test = train_test_split(
        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)

    # Random Forest
    clf = RandomForestClassifier(n_estimators=100, max_depth=4, random_state=42, bootstrap=False)
    clf.fit(X_train, y_train)

    # Prédiction
    y_pred_test = clf.predict(X_test)
    y_pred_val = clf.predict(X_val)

    acc_test = accuracy_score(y_test, y_pred_test)
    acc_val = accuracy_score(y_val, y_pred_val)
    conf_matrix = confusion_matrix(y_test, y_pred_test)

    print(f"\n🔵 Torque = {te}, Speed = {speed}")
    print(f"✅ Validation Accuracy: {acc_val:.4f}")
    print(f"✅ Test Accuracy: {acc_test:.4f}")

    # Confusion Matrix
    ax_cm = axes_cm[idx]

    # Normaliser la matrice de confusion en pourcentages (par ligne)
    cm_percentage = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis] * 100

    # Appliquer la colormap sur les pourcentages
    im = ax_cm.imshow(cm_percentage, interpolation='nearest', cmap=plt.cm.Blues)

    # Ajouter les pourcentages dans chaque cellule
    for i in range(cm_percentage.shape[0]):
        for j in range(cm_percentage.shape[1]):
            ax_cm.text(j, i, f"{format(cm_percentage[i, j], '.2f')}%",
                       ha="center", va="center",
                       color="white" if cm_percentage[i, j] > 50 else "black",  # Couleur du texte en fonction de la luminosité
                       fontsize=11, fontweight='bold')

    # Configurer axes et titres
    ax_cm.set(xticks=np.arange(len(encoder.classes_)),
              yticks=np.arange(len(encoder.classes_)),
              xticklabels=encoder.classes_,
              yticklabels=encoder.classes_)
    ax_cm.set_title(f"Confusion Matrix - Torque={te} Nm, Speed={speed} RPM", fontsize=11, fontweight='bold')
    ax_cm.set_xlabel("Predicted class", fontsize=11, fontweight='bold')
    ax_cm.set_ylabel("True class", fontsize=11, fontweight='bold')

plt.tight_layout()
plt.show()

import scipy.io
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt

# Charger les fichiers MAT
file_faulty = "harmonics_data_refilteredH.mat"
file_healthy = "harmonics_data_nofault_refiltered.mat"

results_faulty = scipy.io.loadmat(file_faulty)["dataset_refiltered"]
results_healthy = scipy.io.loadmat(file_healthy)["dataset_nofault_refiltered"]

# Convertir en DataFrames
df_faulty = pd.DataFrame(results_faulty, columns=['Te', 'Nf', 'speed_i', 'Ia', 'Iq_2nd_inst', 'Iq_4th_inst','severe', 'severity'])
df_healthy = pd.DataFrame(results_healthy, columns=['Te', 'speed_i', 'Iq_2nd_inst', 'Iq_4th_inst'])

df_faulty['label'] = 1
df_healthy['label'] = 0
df_healthy['severity'] = 0

df_combined = pd.concat([df_faulty, df_healthy], ignore_index=True)

# Encoder les labels
from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
df_combined['label'] = encoder.fit_transform(df_combined['label'])

# 🔥 Filtrer les données pour ne garder que celles où Te == 0.00 Nm
df_combined = df_combined[df_combined['Te'] == 0.00].copy()

# Identifier les cas uniques de speed (500 RPM et 1000 RPM)
unique_conditions = df_combined[['Te', 'speed_i']].drop_duplicates()

# Préparation des plots
fig_cm, axes_cm = plt.subplots(1, 2, figsize=(12, 10))  # 2 plots (1x2 pour 500 RPM et 1000 RPM)
axes_cm = axes_cm.flatten()

fig_tree, axes_tree = plt.subplots(1, 2, figsize=(20, 15))  # 2 plots pour les arbres de décision
axes_tree = axes_tree.flatten()

for idx, (te, speed) in enumerate(unique_conditions.values):
    df_case = df_combined[(df_combined['Te'] == te) & (df_combined['speed_i'] == speed)]
    if df_case.empty:
        continue

    # Équilibrer les données pour chaque classe
    min_per_label = df_case['label'].value_counts().min()
    df_balanced = df_case.groupby('label', group_keys=False).apply(
        lambda x: x.sample(min(len(x), min_per_label), random_state=42)).reset_index(drop=True)

    X = df_balanced[['Iq_2nd_inst', 'Iq_4th_inst']]
    y = df_balanced['label']

    # Standardisation
    scaler = StandardScaler()
    X = scaler.fit_transform(X)

    # Split 80/10/10
    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)
    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)

    # Modèle ANN
    model = keras.Sequential([
        keras.layers.Input(shape=(X_train.shape[1],)),
        keras.layers.Dense(20, activation="relu"),
        keras.layers.Dense(20, activation="relu"),
        keras.layers.Dense(20, activation="relu"),
        keras.layers.Dense(1, activation="sigmoid")
    ])

    model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])
    model.fit(X_train, y_train, epochs=15, batch_size=8, verbose=1, validation_data=(X_val, y_val))

    y_pred = (model.predict(X_test) > 0.5).astype("int32")

    print("\n✅ Validation Accuracy:", model.evaluate(X_val, y_val, verbose=0)[1])
    print("✅ Test Accuracy:", accuracy_score(y_test, y_pred))

    cm = confusion_matrix(y_test, y_pred)

    # Confusion Matrix
    ax_cm = axes_cm[idx]

    # Normaliser la matrice de confusion en pourcentages (par ligne)
    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100

    # Appliquer la colormap sur les pourcentages
    im = ax_cm.imshow(cm_percentage, interpolation='nearest', cmap=plt.cm.Blues)

    # Ajouter les pourcentages dans chaque cellule
    for i in range(cm_percentage.shape[0]):
        for j in range(cm_percentage.shape[1]):
            ax_cm.text(j, i, f"{format(cm_percentage[i, j], '.2f')}%",
                       ha="center", va="center",
                       color="white" if cm_percentage[i, j] > 50 else "black",  # Couleur du texte en fonction de la luminosité
                       fontsize=11, fontweight='bold')

    ax_cm.set(
        xticks=np.arange(len(encoder.classes_)),
        yticks=np.arange(len(encoder.classes_)),
        xticklabels=encoder.classes_,
        yticklabels=encoder.classes_
    )
    ax_cm.set_title(f"Confusion Matrix - Torque={te} Nm, Speed={speed} RPM", fontsize=11, fontweight="bold")
    ax_cm.set_xlabel("Predicted class", fontsize=11, fontweight='bold')
    ax_cm.set_ylabel("True class", fontsize=11, fontweight='bold')

plt.tight_layout()
plt.show()

import scipy.io
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, confusion_matrix
import tensorflow as tf
from tensorflow import keras

# 📂 Charger les fichiers
file_faulty = "harmonics_data_refilteredH.mat"
file_healthy = "harmonics_data_nofault_refiltered.mat"

results_faulty = scipy.io.loadmat(file_faulty)["dataset_refiltered"]
results_healthy = scipy.io.loadmat(file_healthy)["dataset_nofault_refiltered"]

# 📊 Conversion en DataFrame
df_faulty = pd.DataFrame(results_faulty, columns=['Te', 'Nf', 'speed_i', 'Ia',
                                                  'Iq_2nd_inst', 'Iq_4th_inst', 'severe', 'severity'])
df_healthy = pd.DataFrame(results_healthy, columns=['Te', 'speed_i', 'Iq_2nd_inst', 'Iq_4th_inst'])
df_faulty["label"] = df_faulty["Nf"] * df_faulty["Ia"]
df_healthy["label"] = 0
df_healthy["severity"] = 0

# 🧩 Fusion
df_combined = pd.concat([df_faulty, df_healthy], ignore_index=True)

# 🌀 Encoder labels
label_encoder = LabelEncoder()
df_combined['label_encoded'] = label_encoder.fit_transform(df_combined['label'])
severity_classes = label_encoder.classes_

# 🔥 Filtrer les données pour ne garder que celles où Te == 0.00 Nm
df_combined = df_combined[df_combined['Te'] == 0.00].copy()

# 📍 Cas (Te, Speed)
unique_conditions = df_combined[["Te", "speed_i"]].drop_duplicates()
conf_matrices = {}

for te, speed in unique_conditions.values:
    df_case = df_combined[(df_combined["Te"] == te) & (df_combined["speed_i"] == speed)]
    if df_case.empty: continue

    # ⚖️ Équilibrer toutes les classes
    min_count = df_case['label_encoded'].value_counts().min()
    df_balanced = df_case.groupby('label_encoded', group_keys=False).apply(lambda x: x.sample(min_count, random_state=42))

    X = df_balanced[["Iq_2nd_inst", "Iq_4th_inst"]].values
    y = df_balanced["label_encoded"].values

    X = StandardScaler().fit_transform(X)

    # 🔀 Split 80/10/10
    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)
    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)

    num_classes = len(np.unique(y))
    model = keras.Sequential([
        keras.layers.Input(shape=(X_train.shape[1],)),
        keras.layers.Dense(20, activation="relu"),
        keras.layers.Dense(20, activation="relu"),
        keras.layers.Dense(20, activation="relu"),
        keras.layers.Dense(num_classes, activation="softmax")
    ])

    model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])
    model.fit(X_train, y_train, epochs=15, batch_size=8, verbose=0, validation_data=(X_val, y_val))

    # 🔍 Évaluation
    val_acc = model.evaluate(X_val, y_val, verbose=0)[1]
    y_pred = np.argmax(model.predict(X_test), axis=1)
    test_acc = accuracy_score(y_test, y_pred)

    print(f"\n🔧 Torque={te}, Speed={speed}")
    print(f"✅ Validation Accuracy: {val_acc:.4f}")
    print(f"✅ Test Accuracy: {test_acc:.4f}")

    cm = confusion_matrix(y_test, y_pred)
    conf_matrices[(te, speed)] = cm

# 📊 Affichage des matrices de confusion en pourcentage
fig, axes = plt.subplots(1, 2, figsize=(13, 10))  # 2 plots (1x2 pour 500 RPM et 1000 RPM)
axes = axes.flatten()

for idx, ((te, speed), cm) in enumerate(conf_matrices.items()):
    ax = axes[idx]

    # Normalisation de la matrice de confusion en pourcentage
    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100

    # Appliquer la colormap sur les pourcentages
    im = ax.imshow(cm_percentage, cmap=plt.cm.Blues)

    # Ajouter les pourcentages dans les cases
    for i in range(cm_percentage.shape[0]):
        for j in range(cm_percentage.shape[1]):
            ax.text(j, i, f"{format(cm_percentage[i, j], '.2f')}%",  # Afficher en pourcentage
                    ha="center", va="center",
                    color="white" if cm_percentage[i, j] > 50 else "black",  # Choisir la couleur du texte
                    fontsize=11, fontweight='bold')

    ticks = [str(val) for val in severity_classes[:cm.shape[0]]]
    ax.set_xticks(np.arange(cm.shape[1]))
    ax.set_yticks(np.arange(cm.shape[0]))
    ax.set_xticklabels(ticks)
    ax.set_yticklabels(ticks)
    ax.set_xlabel("Predicted severity", fontsize=12, fontweight='bold')
    ax.set_ylabel("True severity", fontsize=12, fontweight='bold')
    ax.set_title(f"Confusion Matrix - Torque={te} Nm, Speed={speed} RPM", fontsize=13, fontweight='bold')

plt.tight_layout()
plt.show()